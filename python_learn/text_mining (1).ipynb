{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/gdrive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (6,6)\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints\n",
    "\n",
    "from keras.layers import Dense, Input, LSTM, Bidirectional, Activation, Conv1D, GRU, TimeDistributed\n",
    "from keras.layers import Dropout, Embedding, GlobalMaxPooling1D, MaxPooling1D, Add, Flatten, SpatialDropout1D\n",
    "from keras.layers import GlobalAveragePooling1D, BatchNormalization, concatenate\n",
    "from keras.layers import Reshape, merge, Concatenate, Lambda, Average\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.initializers import Constant\n",
    "from keras.layers.merge import add\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.utils import np_utils\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/content/gdrive/My Drive/data_science/text_mining/train_values.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['doc_text'])\n",
    "X = tokenizer.texts_to_sequences(df['doc_text'])\n",
    "df['words'] = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['word_length'] = df.words.apply(lambda i: len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 50\n",
    "X = list(sequence.pad_sequences(df.words, maxlen=maxlen))\n",
    "#參考資料:https://www.smwenku.com/a/5c113708bd9eee5e40bb23af/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 50\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('/content/gdrive/My Drive/data_science/text_mining/glove.6B.50d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "print('Total %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "embedding_layer = Embedding(len(word_index)+1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=maxlen,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=pd.read_csv('/content/gdrive/My Drive/data_science/text_mining/train_labels.csv',sep=',')\n",
    "#Y的欄名:['row_id', 'information_and_communication_technologies', 'governance',\n",
    "#       'urban_development', 'law_and_development', 'public_sector_development',\n",
    "#       'agriculture', 'communities_and_human_settlements',\n",
    "#       'health_and_nutrition_and_population', 'culture_and_development',\n",
    "#       'environment', 'social_protections_and_labor', 'industry',\n",
    "#       'macroeconomics_and_economic_growth',\n",
    "#       'international_economics_and_trade', 'conflict_and_development',\n",
    "#       'finance_and_financial_sector_development',\n",
    "#       'science_and_technology_development', 'rural_development',\n",
    "#       'poverty_reduction', 'private_sector_development', 'informatics',\n",
    "#       'energy', 'social_development', 'water_resources', 'education',\n",
    "#       'transport', 'water_supply_and_sanitation', 'gender',\n",
    "#       'infrastructure_economics_and_finance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=Y.drop('row_id',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "\n",
    "\n",
    "# and split to training set and validation set\n",
    "\n",
    "seed = 15\n",
    "x_train, x_val, y_train, y_val = train_test_split(X, Y, test_size=0.16, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(maxlen,), dtype='int32')\n",
    "x = embedding_layer(inp)\n",
    "x = SpatialDropout1D(0.2)(x)\n",
    "x = Bidirectional(GRU(256, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\n",
    "x = Conv1D(64, kernel_size=3)(x)\n",
    "avg_pool = GlobalAveragePooling1D()(x)\n",
    "max_pool = GlobalMaxPooling1D()(x)\n",
    "x = concatenate([avg_pool, max_pool])\n",
    "outp = Dense(Y.shape[1], activation=\"softmax\")(x)\n",
    "\n",
    "BiGRU = Model(inp, outp)\n",
    "BiGRU.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigru_history = BiGRU.fit(x_train, \n",
    "                          y_train, \n",
    "                          batch_size=128, \n",
    "                          epochs=40, \n",
    "                          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted1 = BiGRU.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save=pd.DataFrame(predicted1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cluster\n",
    "for j in tqdm(range(0,save.shape[0])):\n",
    "  dataf=pd.DataFrame(save.iloc[j,:])\n",
    "  #dataf['other']=0\n",
    "  temp=np.array(dataf)\n",
    "  kmeans_fit = cluster.KMeans(n_clusters = 2).fit(temp)\n",
    "  re_temp=kmeans_fit.predict(temp)\n",
    "  for ii in range (0,29):\n",
    "    f_result_copy.iloc[j,ii]=int(re_temp[ii])\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=0\n",
    "for i in range(0,f_result_copy.shape[0]):\n",
    "  if list(f_result_copy.iloc[i,:].astype(int))==list(y_val.iloc[i,:])  :\n",
    "    k=k+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k/f_result_copy.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#用到的:https://drive.google.com/open?id=13Jqaug5zRdwscA82O-UhYoeM5WZ_dK1V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
