{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 1: Decision Tree\n",
      "gini of [0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1]: 0.4628099173553719\n",
      "entropy of [0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1]: 0.9456603046006401\n",
      "Accuracy (gini with max_depth=7): 0.7049180327868853\n",
      "Accuracy (entropy with max_depth=7): 0.7049180327868853\n"
     ]
    }
   ],
   "source": [
    "# You are not allowed to import any additional packages/libraries.\n",
    "import numpy as np\n",
    "from pandas import DataFrame, read_csv\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# This function computes the gini impurity of a label array.\n",
    "def gini(y):\n",
    "  a, counts = np.unique(y, return_counts=True)\n",
    "  probabilities = counts / len(y)\n",
    "  gini = 1 - np.sum(probabilities**2)\n",
    "  return gini\n",
    "\n",
    "# This function computes the entropy of a label array.\n",
    "def entropy(y):\n",
    "  a, counts = np.unique(y, return_counts=True)\n",
    "  probabilities = counts / len(y)\n",
    "  entropy_value = -np.sum(probabilities * np.log2(probabilities))\n",
    "  return entropy_value\n",
    "\n",
    "\n",
    "# The decision tree classifier class.\n",
    "# Tips: You may need another node class and build the decision tree recursively.\n",
    "class DecisionTree():\n",
    "    def __init__(self, criterion='gini', max_depth=None):\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "\n",
    "    def gini(self,y):\n",
    "      a, counts = np.unique(y, return_counts=True)\n",
    "      probabilities = counts / len(y)\n",
    "      gini = 1 - np.sum(probabilities**2)\n",
    "      return gini\n",
    "\n",
    "# This function computes the entropy of a label array.\n",
    "    def entropy(self,y):\n",
    "      a, counts = np.unique(y, return_counts=True)\n",
    "      probabilities = counts / len(y)\n",
    "      entropy_value = -np.sum(probabilities * np.log2(probabilities))\n",
    "      return entropy_value\n",
    "    # This function computes the impurity based on the criterion.\n",
    "    def impurity(self, y):\n",
    "        if self.criterion == 'gini':\n",
    "            return gini(y)\n",
    "        elif self.criterion == 'entropy':\n",
    "            return entropy(y)\n",
    "\n",
    "\n",
    "    # This function fits the given data using the decision tree algorithm.\n",
    "    def fit(self, X, y, depth=0):\n",
    "        num_samples, num_features = X.shape\n",
    "        unique_classes = np.unique(y)\n",
    "\n",
    "        # If only one class in the data or max depth reached, return a leaf node\n",
    "        if len(unique_classes) == 1 or (self.max_depth is not None and depth == self.max_depth):\n",
    "            return {'class': unique_classes[0]}\n",
    "\n",
    "        # Find the best split\n",
    "        best_split = self._find_best_split(X, y)\n",
    "\n",
    "        # If no split is found, return a leaf node\n",
    "        if best_split is None:\n",
    "            return {'class': np.bincount(y).argmax()}\n",
    "\n",
    "        # Split the data\n",
    "        left_mask = X[:, best_split['feature']] <= best_split['threshold']\n",
    "        right_mask = ~left_mask\n",
    "\n",
    "        # Recursively build left and right subtrees\n",
    "        left_subtree = self.fit(X[left_mask], y[left_mask], depth + 1)\n",
    "        right_subtree = self.fit(X[right_mask], y[right_mask], depth + 1)\n",
    "        self.tree={\n",
    "            'feature': best_split['feature'],\n",
    "            'threshold': best_split['threshold'],\n",
    "            'left': left_subtree,\n",
    "            'right': right_subtree\n",
    "        }\n",
    "        return self.tree\n",
    "        # return {\n",
    "        #     'feature': best_split['feature'],\n",
    "        #     'threshold': best_split['threshold'],\n",
    "        #     'left': left_subtree,\n",
    "        #     'right': right_subtree\n",
    "        # }\n",
    "\n",
    "    def _find_best_split(self, X, y):\n",
    "        num_samples, num_features = X.shape\n",
    "        best_split = None\n",
    "        best_gini = float('inf')\n",
    "\n",
    "        for feature in range(num_features):\n",
    "            unique_values = np.unique(X[:, feature])\n",
    "            for value in unique_values:\n",
    "                left_mask = X[:, feature] <= value\n",
    "                right_mask = ~left_mask\n",
    "\n",
    "                left_gini = self.gini(y[left_mask])\n",
    "                right_gini = self.gini(y[right_mask])\n",
    "                gini = (len(y[left_mask]) / num_samples) * left_gini + (len(y[right_mask]) / num_samples) * right_gini\n",
    "                if gini < best_gini:\n",
    "                    best_gini = gini\n",
    "                    best_split = {'feature': feature, 'threshold': value}\n",
    "\n",
    "        return best_split\n",
    "\n",
    "\n",
    "    # This function takes the input data X and predicts the class label y according to your trained model.\n",
    "    def predict(self, X):\n",
    "      return np.array([self._predict_tree(x, self.tree) for x in X])\n",
    "    def _predict_tree(self, x, node):\n",
    "      if 'class' in node:\n",
    "        return node['class']\n",
    "      if x[node['feature']] <= node['threshold']:\n",
    "        return self._predict_tree(x, node['left'])\n",
    "      else:\n",
    "        return self._predict_tree(x, node['right'])\n",
    "\n",
    "    # This function plots the feature importance of the decision tree.\n",
    "    def plot_feature_importance_img(self, columns):\n",
    "        if self.tree is None:\n",
    "            raise RuntimeError(\"The model has not been trained. Call fit() before plotting feature importance.\")\n",
    "\n",
    "        feature_importance = self._calculate_feature_importance(self.tree,columns)\n",
    "        sorted_idx = np.argsort(feature_importance)\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.barh(range(len(sorted_idx)), feature_importance[sorted_idx])\n",
    "        plt.yticks(range(len(sorted_idx)), np.array(columns)[sorted_idx])\n",
    "        plt.xlabel('Feature Importance')\n",
    "        plt.ylabel('Feature')\n",
    "        plt.title('Decision Tree Feature Importance')\n",
    "        plt.show()\n",
    "\n",
    "    def _calculate_feature_importance(self, tree,columns):\n",
    "        if 'class' in tree:\n",
    "            return np.zeros_like(columns, dtype=float)\n",
    "\n",
    "        importance = np.zeros_like(columns, dtype=float)\n",
    "        importance[tree['feature']] += 1\n",
    "\n",
    "        importance_left = self._calculate_feature_importance(tree['left'],columns)\n",
    "        importance_right = self._calculate_feature_importance(tree['right'],columns)\n",
    "\n",
    "        return importance + importance_left + importance_right\n",
    "\n",
    "# The AdaBoost classifier class.\n",
    "class AdaBoost():\n",
    "    def __init__(self, criterion='gini', n_estimators=200):\n",
    "        self.criterion = criterion\n",
    "        self.n_estimators = n_estimators\n",
    "\n",
    "    # This function fits the given data using the AdaBoost algorithm.\n",
    "    # You need to create a decision tree classifier with max_depth = 1 in each iteration.\n",
    "    def fit(self, X, y):\n",
    "        pass\n",
    "\n",
    "    # This function takes the input data X and predicts the class label y according to your trained model.\n",
    "    def predict(self, X):\n",
    "        pass\n",
    "\n",
    "# Do not modify the main function architecture.\n",
    "# You can only modify the value of the random seed and the the arguments of your Adaboost class.\n",
    "if __name__ == \"__main__\":\n",
    "# Data Loading\n",
    "    train_df = DataFrame(read_csv(\"train.csv\"))\n",
    "    test_df = DataFrame(read_csv(\"test.csv\"))\n",
    "    X_train = train_df.drop([\"target\"], axis=1)\n",
    "    y_train = train_df[\"target\"]\n",
    "    X_test = test_df.drop([\"target\"], axis=1)\n",
    "    y_test = test_df[\"target\"]\n",
    "\n",
    "    X_train = X_train.to_numpy()\n",
    "    y_train = y_train.to_numpy()\n",
    "    X_test = X_test.to_numpy()\n",
    "    y_test = y_test.to_numpy()\n",
    "\n",
    "# Set random seed to make sure you get the same result every time.\n",
    "# You can change the random seed if you want to.\n",
    "    np.random.seed(0)\n",
    "\n",
    "# Decision Tree\n",
    "    print(\"Part 1: Decision Tree\")\n",
    "    data = np.array([0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1])\n",
    "    print(f\"gini of [0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1]: {gini(data)}\")\n",
    "    print(f\"entropy of [0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1]: {entropy(data)}\")\n",
    "    tree = DecisionTree(criterion='gini', max_depth=7)\n",
    "    tree.fit(X_train, y_train)\n",
    "    y_pred = tree.predict(X_test)\n",
    "    print(\"Accuracy (gini with max_depth=7):\", accuracy_score(y_test, y_pred))\n",
    "    tree = DecisionTree(criterion='entropy', max_depth=7)\n",
    "    tree.fit(X_train, y_train)\n",
    "    y_pred = tree.predict(X_test)\n",
    "    print(\"Accuracy (entropy with max_depth=7):\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# # AdaBoost\n",
    "#     print(\"Part 2: AdaBoost\")\n",
    "#     # Tune the arguments of AdaBoost to achieve higher accuracy than your Decision Tree.\n",
    "#     ada = AdaBoost(criterion='gini', n_estimators=200)\n",
    "#     ada.fit(X_train, y_train)\n",
    "#     y_pred = ada.predict(X_test)\n",
    "#     print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 1: Decision Tree\n",
      "gini of [0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1]: 0.4628099173553719\n",
      "entropy of [0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1]: 0.9456603046006401\n",
      "Accuracy (gini with max_depth=7): 0.7049180327868853\n",
      "Accuracy (entropy with max_depth=7): 0.7049180327868853\n",
      "Part 2: AdaBoost\n",
      "Accuracy: 0.5081967213114754\n"
     ]
    }
   ],
   "source": [
    "# You are not allowed to import any additional packages/libraries.\n",
    "import numpy as np\n",
    "from pandas import DataFrame, read_csv\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# This function computes the gini impurity of a label array.\n",
    "def gini(y):\n",
    "    a, counts = np.unique(y, return_counts=True)\n",
    "    probabilities = counts / len(y)\n",
    "    gini = 1 - np.sum(probabilities**2)\n",
    "    return gini\n",
    "\n",
    "# This function computes the entropy of a label array.\n",
    "def entropy(y):\n",
    "    a, counts = np.unique(y, return_counts=True)\n",
    "    probabilities = counts / len(y)\n",
    "    entropy_value = -np.sum(probabilities * np.log2(probabilities))\n",
    "    return entropy_value\n",
    "        \n",
    "# The decision tree classifier class.\n",
    "# Tips: You may need another node class and build the decision tree recursively.\n",
    "class DecisionTree():\n",
    "    def __init__(self, criterion='gini', max_depth=None):\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth \n",
    "        self.tree = None\n",
    "    \n",
    "    # This function computes the impurity based on the criterion.\n",
    "    def impurity(self, y):\n",
    "        if self.criterion == 'gini':\n",
    "            return gini(y)\n",
    "        elif self.criterion == 'entropy':\n",
    "            return entropy(y)\n",
    "    def gini(self,y):\n",
    "        a, counts = np.unique(y, return_counts=True)\n",
    "        probabilities = counts / len(y)\n",
    "        gini = 1 - np.sum(probabilities**2)\n",
    "        return gini\n",
    "    def entropy(self,y):\n",
    "        a, counts = np.unique(y, return_counts=True)\n",
    "        probabilities = counts / len(y)\n",
    "        entropy_value = -np.sum(probabilities * np.log2(probabilities))\n",
    "        return entropy_value\n",
    "    # This function fits the given data using the decision tree algorithm.\n",
    "    def fit(self, X, y,depth=0):\n",
    "        num_samples, num_features = X.shape\n",
    "        unique_classes = np.unique(y)\n",
    "        if len(unique_classes) == 1 or (self.max_depth is not None and depth == self.max_depth):\n",
    "            return {'class': unique_classes[0]}\n",
    "        best_split = self._find_best_split(X, y)\n",
    "        if best_split is None:\n",
    "            return {'class': np.bincount(y).argmax()}\n",
    "        left_mask = X[:, best_split['feature']] <= best_split['threshold']\n",
    "        right_mask = ~left_mask\n",
    "        left_subtree = self.fit(X[left_mask], y[left_mask], depth + 1)\n",
    "        right_subtree = self.fit(X[right_mask], y[right_mask], depth + 1)\n",
    "        self.tree={\n",
    "            'feature': best_split['feature'],\n",
    "            'threshold': best_split['threshold'],\n",
    "            'left': left_subtree,\n",
    "            'right': right_subtree\n",
    "        }\n",
    "        return self.tree\n",
    "    def _find_best_split(self, X, y):\n",
    "        num_samples, num_features = X.shape\n",
    "        best_split = None\n",
    "        best_gini = float('inf')\n",
    "\n",
    "        for feature in range(num_features):\n",
    "            unique_values = np.unique(X[:, feature])\n",
    "            for value in unique_values:\n",
    "                left_mask = X[:, feature] <= value\n",
    "                right_mask = ~left_mask\n",
    "                left_gini = self.gini(y[left_mask])\n",
    "                right_gini = self.gini(y[right_mask])\n",
    "                gini = (len(y[left_mask]) / num_samples) * left_gini + (len(y[right_mask]) / num_samples) * right_gini\n",
    "                if gini < best_gini:\n",
    "                    best_gini = gini\n",
    "                    best_split = {'feature': feature, 'threshold': value}\n",
    "\n",
    "        return best_split\n",
    "    # This function takes the input data X and predicts the class label y according to your trained model.\n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict_tree(x, self.tree) for x in X])\n",
    "    def _predict_tree(self, x, node):\n",
    "        if 'class' in node:\n",
    "            return node['class']\n",
    "        if x[node['feature']] <= node['threshold']:\n",
    "            return self._predict_tree(x, node['left'])\n",
    "        else:\n",
    "            return self._predict_tree(x, node['right'])\n",
    "    # This function plots the feature importance of the decision tree.\n",
    "    def plot_feature_importance_img(self, columns):\n",
    "        pass\n",
    "\n",
    "# The AdaBoost classifier class.\n",
    "class Decision:\n",
    "    def __init__(self,feature,threshold,polarity):\n",
    "        self.feature =feature\n",
    "        self.threshold=threshold\n",
    "        self.polarity=polarity\n",
    "    def predict(self,X):\n",
    "        return self.polarity*(X[:,self.feature]>self.threshold)\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=None):\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    def fit(self, X, y, weights):\n",
    "        self.feature, self.threshold, self.polarity = self.create_weak_learner(X, y, weights)\n",
    "\n",
    "    def create_weak_learner(self, X, y, weights):\n",
    "        m, n = X.shape\n",
    "        best_feature = 0\n",
    "        threshold = 0\n",
    "        polarity = 1\n",
    "        min_error = float('inf')\n",
    "\n",
    "        for feature in range(n):\n",
    "            unique_values = np.unique(X[:, feature])\n",
    "            for value in unique_values:\n",
    "                for sign in [1, -1]:\n",
    "                    predictions = sign * np.ones(m)\n",
    "                    error = np.sum(weights * (predictions != y))\n",
    "\n",
    "                    if error < min_error:\n",
    "                        min_error = error\n",
    "                        best_feature = feature\n",
    "                        threshold = value\n",
    "                        polarity = sign\n",
    "\n",
    "        return best_feature, threshold, polarity\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.polarity * (X[:, self.feature] > self.threshold)\n",
    "\n",
    "class AdaBoost():\n",
    "    def __init__(self, criterion='gini', n_estimators=200,max_depth=None):\n",
    "        self.criterion = criterion \n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.alphas=[]\n",
    "        self.DTs=[]\n",
    "\n",
    "    # This function fits the given data using the AdaBoost algorithm.\n",
    "    # You need to create a decision tree classifier with max_depth = 1 in each iteration.\n",
    "    def fit(self, X, y):\n",
    "        m=len(y)\n",
    "        weights=np.ones(m)/m\n",
    "        for i in range(self.n_estimators):\n",
    "            DT=self.create_weak_learner(X,y,weights)\n",
    "            predictions = DT.predict(X)\n",
    "            weighted_e=np.sum(weights*(predictions!=y))\n",
    "            alpha =0.9*np.log((1-weighted_e)/max(weighted_e,1e-10))\n",
    "            weights=weights*np.exp(-alpha*y*predictions)\n",
    "            weights/=np.sum(weights)\n",
    "            self.alphas.append(alpha)\n",
    "            self.DTs.append(DT)\n",
    "    def create_weak_learner(self,X,y,weights):\n",
    "        m,n=X.shape\n",
    "        best_feature=0\n",
    "        threshold=0\n",
    "        polarity=1\n",
    "        min_error=float('inf')\n",
    "        for feature in range(n):\n",
    "            unique_values=np.unique(X[:,feature])\n",
    "            for value in unique_values:\n",
    "                for sign in [1,-1]:\n",
    "                    predictions =sign*np.ones(m)\n",
    "                    error =np.sum(weights*(predictions!=y))\n",
    "                    if error<min_error:\n",
    "                        min_error=error\n",
    "                        best_feature=feature\n",
    "                        threshold=value\n",
    "                        polarity=sign\n",
    "        return Decision(feature=best_feature,threshold=threshold,polarity=polarity)\n",
    "        # {'feature':best_feature,'threshold':threshold,'polarity':polarity}\n",
    "    def predict(self,X):\n",
    "        predictions=np.zeros(len(X))\n",
    "        for alpha ,DT in zip(self.alphas,self.DTs):\n",
    "            predictions +=alpha*DT.predict(X)\n",
    "        return np.sign(predictions).astype(int)\n",
    "\n",
    "    # This function takes the input data X and predicts the class label y according to your trained model.\n",
    "\n",
    "\n",
    "# Do not modify the main function architecture.\n",
    "# You can only modify the value of the random seed and the the arguments of your Adaboost class.\n",
    "if __name__ == \"__main__\":\n",
    "# Data Loading\n",
    "    train_df = DataFrame(read_csv(\"train.csv\"))\n",
    "    test_df = DataFrame(read_csv(\"test.csv\"))\n",
    "    X_train = train_df.drop([\"target\"], axis=1)\n",
    "    y_train = train_df[\"target\"]\n",
    "    X_test = test_df.drop([\"target\"], axis=1)\n",
    "    y_test = test_df[\"target\"]\n",
    "\n",
    "    X_train = X_train.to_numpy()\n",
    "    y_train = y_train.to_numpy()\n",
    "    X_test = X_test.to_numpy()\n",
    "    y_test = y_test.to_numpy()\n",
    "\n",
    "# Set random seed to make sure you get the same result every time.\n",
    "# You can change the random seed if you want to.\n",
    "    np.random.seed(0)\n",
    "\n",
    "# Decision Tree\n",
    "    print(\"Part 1: Decision Tree\")\n",
    "    data = np.array([0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1])\n",
    "    print(f\"gini of [0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1]: {gini(data)}\")\n",
    "    print(f\"entropy of [0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1]: {entropy(data)}\")\n",
    "    tree = DecisionTree(criterion='gini', max_depth=7)\n",
    "    tree.fit(X_train, y_train)\n",
    "    y_pred = tree.predict(X_test)\n",
    "    print(\"Accuracy (gini with max_depth=7):\", accuracy_score(y_test, y_pred))\n",
    "    tree = DecisionTree(criterion='entropy', max_depth=7)\n",
    "    tree.fit(X_train, y_train)\n",
    "    y_pred = tree.predict(X_test)\n",
    "    print(\"Accuracy (entropy with max_depth=7):\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# AdaBoost\n",
    "    print(\"Part 2: AdaBoost\")\n",
    "    # Tune the arguments of AdaBoost to achieve higher accuracy than your Decision Tree.\n",
    "    ada = AdaBoost(criterion='gini', n_estimators=1000)\n",
    "    ada.fit(X_train, y_train)\n",
    "    y_pred = ada.predict(X_test)\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
